<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial &mdash; RaNNC 0.7.5 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Limitations" href="limitations.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> RaNNC
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#steps-to-use-rannc">Steps to use RaNNC</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#set-up-environment">0. Set up environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#import-rannc">1. Import RaNNC</a></li>
<li class="toctree-l3"><a class="reference internal" href="#wrap-your-model">2. Wrap your model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-forward-backward-passes">3. Run forward/backward passes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#launch-with-a-small-model">4. Launch (with a small model)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-partitioning-for-very-large-models">5. Model partitioning for very large models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">API references</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="config.html">Configurations</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building from source</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">RaNNC</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Tutorial</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tutorial.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this heading"></a></h1>
<p>The greatest feature of RaNNC is that it can automatically partition a model written for PyTorch and train it using
multiple GPUs (model parallelism).
Unlike other frameworks for model parallelism, users do not need to modify the model for partitioning.</p>
<p>In this tutorial, you will learn how to use RaNNC to train very large models that cannot be trained using data parallelism only.</p>
<section id="steps-to-use-rannc">
<h2>Steps to use RaNNC<a class="headerlink" href="#steps-to-use-rannc" title="Permalink to this heading"></a></h2>
<section id="set-up-environment">
<h3>0. Set up environment<a class="headerlink" href="#set-up-environment" title="Permalink to this heading"></a></h3>
<p>Ensure the required tools and libraries (CUDA, NCCL, OpenMPI, etc.) are available in your environment.
The libraries must be included in <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> at runtime.
Then install <code class="docutils literal notranslate"><span class="pre">pyrannc</span></code> following the commands shown in <a class="reference internal" href="installation.html"><span class="doc">Installation</span></a> page.</p>
</section>
<section id="import-rannc">
<h3>1. Import RaNNC<a class="headerlink" href="#import-rannc" title="Permalink to this heading"></a></h3>
<p>Insert <code class="docutils literal notranslate"><span class="pre">import</span></code> in your script.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyrannc</span>
</pre></div>
</div>
</section>
<section id="wrap-your-model">
<h3>2. Wrap your model<a class="headerlink" href="#wrap-your-model" title="Permalink to this heading"></a></h3>
<p>Wrap your model by using <code class="docutils literal notranslate"><span class="pre">pyrannc.RaNNCModule</span></code> with your optimizer.
You can use the wrapped model in almost the same manner as the original model (see below).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">pyrannc</span><span class="o">.</span><span class="n">RaNNCModule</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the original model does not need to be on a CUDA device.
Thus you can declare a very large model that does not fit to the memory of a GPU.</p>
<p>If you do not use an optimizer, pass only the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">pyrannc</span><span class="o">.</span><span class="n">RaNNCModule</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="run-forward-backward-passes">
<h3>3. Run forward/backward passes<a class="headerlink" href="#run-forward-backward-passes" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">RaNNCModule</span></code> can run forward/backward passes, as with <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
<p>Inputs to <code class="docutils literal notranslate"><span class="pre">RaNNCModule</span></code> must be CUDA tensors.
RaNNCModule has several more limitations regarding a wrapped model and inputs/outputs.
See <a class="reference internal" href="limitations.html"><span class="doc">Limitations</span></a> for details.
The optimizer can update model parameters simply by calling <code class="docutils literal notranslate"><span class="pre">step()</span></code>.</p>
<p>The <a class="reference external" href="https://github.com/nict-wisdom/rannc/blob/main/examples/tutorial.py">script below</a> shows the usage with a very simple model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">pyrannc</span>


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">layers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
<span class="k">if</span> <span class="n">pyrannc</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;#Parameters=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())))</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">pyrannc</span><span class="o">.</span><span class="n">RaNNCModule</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Finished on rank</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pyrannc</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section id="launch-with-a-small-model">
<h3>4. Launch (with a small model)<a class="headerlink" href="#launch-with-a-small-model" title="Permalink to this heading"></a></h3>
<p>A program using RaNNC must be launched using <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>.
Begin with launching the above script with a very small model using two GPUs.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># The arguments are: [batch_size] [hidden] [layers]</span>
mpirun -np <span class="m">2</span> python tutorial.py <span class="m">64</span> <span class="m">512</span> <span class="m">10</span>
</pre></div>
</div>
<p>(Ensure MPI is properly configured in your environment before you run RaNNC. You may need more options for MPI like
<code class="docutils literal notranslate"><span class="pre">--mca</span> <span class="pre">pml</span> <span class="pre">ucx</span> <span class="pre">--mca</span> <span class="pre">btl</span> <span class="pre">^vader,tcp,openib</span> <span class="pre">...</span></code>)</p>
<p><code class="docutils literal notranslate"><span class="pre">-np</span></code> indicates the number of processes (ranks).
RaNNC allocates one CUDA device for each process.
In the above example, there must be two available CUDA devices.
By properly setting nodes for MPI, you can run processes using RaNNC across multiple nodes
(Ensure that you have the equal or more number of GPUs than processes).</p>
<p>The following shows the output on our compute node that has eight NVIDIA A100’s (40GB memory).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ mpirun -np <span class="m">2</span> --mca pml ucx --mca btl ^vader,tcp,openib --mca coll ^hcoll python tutorial.py <span class="m">64</span> <span class="m">512</span> <span class="m">10</span>
<span class="o">[</span>RaNNCProcess<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> RaNNC started on rank <span class="m">1</span> <span class="o">(</span>gpunode001<span class="o">)</span>
<span class="o">[</span>RaNNCProcess<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> RaNNC started on rank <span class="m">0</span> <span class="o">(</span>gpunode001<span class="o">)</span>
<span class="o">[</span>RaNNCProcess<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> CUDA device assignments:
<span class="o">[</span>RaNNCProcess<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span>  Worker <span class="m">0</span>: device0@gpunode001
<span class="o">[</span>RaNNCProcess<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span>  Worker <span class="m">1</span>: device1@gpunode001
<span class="c1">#Parameters=2626560</span>
<span class="o">[</span>RaNNCModule<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Tracing model ...
<span class="o">[</span>RaNNCModule<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Converting torch model to IR ...
<span class="o">[</span>RaNNCModule<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Running profiler ...
<span class="o">[</span>RaNNCModule<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Profiling finished
<span class="o">[</span>RaNNCModule<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Assuming batch size: <span class="m">128</span>
<span class="o">[</span>Decomposer<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Decomposer: ml_part
<span class="o">[</span>Decomposer<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Available device memory: <span class="m">38255689728</span>
<span class="o">[</span>Decomposer<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Starting model partitioning ... <span class="o">(</span>this may take a very long <span class="nb">time</span><span class="o">)</span>
<span class="o">[</span>DPStaging<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Estimated profiles of subgraphs: <span class="nv">batch_size</span><span class="o">=</span><span class="m">128</span> <span class="nv">np</span><span class="o">=</span><span class="m">2</span> <span class="nv">pipeline</span><span class="o">=</span><span class="m">1</span> <span class="nv">use_amp</span><span class="o">=</span><span class="m">0</span> <span class="nv">zero</span><span class="o">=</span><span class="m">0</span>
  <span class="nv">graph</span><span class="o">=</span>MERGE_0_9 <span class="nv">repl</span><span class="o">=</span><span class="m">2</span> <span class="nv">fwd_time</span><span class="o">=</span><span class="m">4722</span> <span class="nv">bwd_time</span><span class="o">=</span><span class="m">24237</span> <span class="nv">ar_time</span><span class="o">=</span><span class="m">978</span> <span class="nv">in_size</span><span class="o">=</span><span class="m">131072</span> <span class="nv">out_size</span><span class="o">=</span><span class="m">131072</span> <span class="nv">fp32param_size</span><span class="o">=</span><span class="m">10506240</span> <span class="nv">fp16param_size</span><span class="o">=</span><span class="m">0</span> <span class="nv">total_mem</span><span class="o">=</span><span class="m">54759424</span> <span class="o">(</span>fwd+bwd<span class="o">=</span><span class="m">33353728</span> <span class="nv">opt</span><span class="o">=</span><span class="m">21012480</span> <span class="nv">comm</span><span class="o">=</span><span class="m">393216</span><span class="o">)</span>
<span class="o">[</span>Decomposer<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span>  Assigned subgraph MERGE_0_9 to rank<span class="o">[</span><span class="m">1</span>,0<span class="o">]</span>
<span class="o">[</span>RaNNCModule<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Routes verification passed.
<span class="o">[</span>ParamStorage<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Synchronizing parameters ...
<span class="o">[</span>RaNNCModule<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> RaNNCModule is ready. <span class="o">(</span>rank0<span class="o">)</span>
<span class="o">[</span>RaNNCModule<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> RaNNCModule is ready. <span class="o">(</span>rank1<span class="o">)</span>
Finished on rank0
Finished on rank1
</pre></div>
</div>
<p>Since this model is very small, RaNNC determines to train it using only data parallelism.
You can see the partitioning result in the following part.
The computational graph that is equivalent to the model was named <code class="docutils literal notranslate"><span class="pre">MERGE_0_9</span></code> and assigned to ranks 0 and 1
(replicated for data parallelism).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>DPStaging<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Estimated profiles of subgraphs: <span class="nv">batch_size</span><span class="o">=</span><span class="m">128</span> <span class="nv">np</span><span class="o">=</span><span class="m">2</span> <span class="nv">pipeline</span><span class="o">=</span><span class="m">1</span> <span class="nv">use_amp</span><span class="o">=</span><span class="m">0</span> <span class="nv">zero</span><span class="o">=</span><span class="m">0</span>
  <span class="nv">graph</span><span class="o">=</span>MERGE_0_9 <span class="nv">repl</span><span class="o">=</span><span class="m">2</span> <span class="nv">fwd_time</span><span class="o">=</span><span class="m">4722</span> <span class="nv">bwd_time</span><span class="o">=</span><span class="m">24237</span> <span class="nv">ar_time</span><span class="o">=</span><span class="m">978</span> <span class="nv">in_size</span><span class="o">=</span><span class="m">131072</span> <span class="nv">out_size</span><span class="o">=</span><span class="m">131072</span> <span class="nv">fp32param_size</span><span class="o">=</span><span class="m">10506240</span> <span class="nv">fp16param_size</span><span class="o">=</span><span class="m">0</span> <span class="nv">total_mem</span><span class="o">=</span><span class="m">54759424</span> <span class="o">(</span>fwd+bwd<span class="o">=</span><span class="m">33353728</span> <span class="nv">opt</span><span class="o">=</span><span class="m">21012480</span> <span class="nv">comm</span><span class="o">=</span><span class="m">393216</span><span class="o">)</span>
<span class="o">[</span>Decomposer<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span>  Assigned subgraph MERGE_0_9 to rank<span class="o">[</span><span class="m">1</span>,0<span class="o">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each process launched by MPI is expected to load different (mini-)batches.
RaNNC automatically gathers the batches from all ranks and computes them as a single batch.
Therefore, the effective (global) batch size is [number of processes (np)] * [batch size per process].
<code class="docutils literal notranslate"><span class="pre">torch.utils.data.distributed.DistributedSampler</span></code> is useful to properly take batches in each process.</p>
</div>
</section>
<section id="model-partitioning-for-very-large-models">
<h3>5. Model partitioning for very large models<a class="headerlink" href="#model-partitioning-for-very-large-models" title="Permalink to this heading"></a></h3>
<p>If the number of parameters of a model is very large, you cannot train the model only with data parallelism.
RaNNC automatically partitions such models for <em>model parallelism</em>.</p>
<p>To see how RaNNC partitions such a large model, set <code class="docutils literal notranslate"><span class="pre">hidden</span></code> and <code class="docutils literal notranslate"><span class="pre">layers</span></code> to 5000 and 100 respectively.
Given the configuration, the model has more than 2.5 billion parameters.</p>
<p>You cannot train this model using only data parallelism because the size of parameters, gradients
and optimizer states exceeds the memory of the GPU (40GB). (The model requires 10GB for parameters, 10GB for gradients,
20GB for optimizer states, and more for activations)</p>
<p>Let’s use all the GPUs on the node (eight GPUs) for this configuration.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ mpirun -np <span class="m">8</span> --mca pml ucx --mca btl ^vader,tcp,openib --mca coll ^hcoll python tutorial.py <span class="m">64</span> <span class="m">5000</span> <span class="m">100</span>
<span class="o">[</span>RaNNCProcess<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> RaNNC started on rank <span class="m">0</span> <span class="o">(</span>gpunode001<span class="o">)</span>
<span class="o">[</span>RaNNCProcess<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> RaNNC started on rank <span class="m">1</span> <span class="o">(</span>gpunode001<span class="o">)</span>
...
<span class="nv">Parameters</span><span class="o">=</span><span class="m">2500500000</span>
..
<span class="o">[</span>Decomposer<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Starting model partitioning ... <span class="o">(</span>this may take a very long <span class="nb">time</span><span class="o">)</span>
<span class="o">[</span>DPStaging<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span> Estimated profiles of subgraphs: <span class="nv">batch_size</span><span class="o">=</span><span class="m">512</span> <span class="nv">np</span><span class="o">=</span><span class="m">8</span> <span class="nv">pipeline</span><span class="o">=</span><span class="m">1</span> <span class="nv">use_amp</span><span class="o">=</span><span class="m">0</span> <span class="nv">zero</span><span class="o">=</span><span class="m">0</span>
<span class="nv">graph</span><span class="o">=</span>MERGE_0_4 <span class="nv">repl</span><span class="o">=</span><span class="m">4</span> <span class="nv">fwd_time</span><span class="o">=</span><span class="m">27516</span> <span class="nv">bwd_time</span><span class="o">=</span><span class="m">126756</span> <span class="nv">ar_time</span><span class="o">=</span><span class="m">437809</span> <span class="nv">in_size</span><span class="o">=</span><span class="m">2560000</span> <span class="nv">out_size</span><span class="o">=</span><span class="m">2560000</span> <span class="nv">fp32param_size</span><span class="o">=</span><span class="m">4700940000</span> <span class="nv">fp16param_size</span><span class="o">=</span><span class="m">0</span> <span class="nv">total_mem</span><span class="o">=</span><span class="m">23707792544</span> <span class="o">(</span>fwd+bwd<span class="o">=</span><span class="m">14298232544</span> <span class="nv">opt</span><span class="o">=</span><span class="m">9401880000</span> <span class="nv">comm</span><span class="o">=</span><span class="m">7680000</span><span class="o">)</span>
<span class="nv">graph</span><span class="o">=</span>MERGE_5_9 <span class="nv">repl</span><span class="o">=</span><span class="m">4</span> <span class="nv">fwd_time</span><span class="o">=</span><span class="m">31228</span> <span class="nv">bwd_time</span><span class="o">=</span><span class="m">153762</span> <span class="nv">ar_time</span><span class="o">=</span><span class="m">493699</span> <span class="nv">in_size</span><span class="o">=</span><span class="m">2560000</span> <span class="nv">out_size</span><span class="o">=</span><span class="m">2560000</span> <span class="nv">fp32param_size</span><span class="o">=</span><span class="m">5301060000</span> <span class="nv">fp16param_size</span><span class="o">=</span><span class="m">0</span> <span class="nv">total_mem</span><span class="o">=</span><span class="m">26732209376</span> <span class="o">(</span>fwd+bwd<span class="o">=</span><span class="m">16122409376</span> <span class="nv">opt</span><span class="o">=</span><span class="m">10602120000</span> <span class="nv">comm</span><span class="o">=</span><span class="m">7680000</span><span class="o">)</span>
<span class="o">[</span>Decomposer<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span>  Assigned subgraph MERGE_5_9 to rank<span class="o">[</span><span class="m">7</span>,5,1,3<span class="o">]</span>
<span class="o">[</span>Decomposer<span class="o">]</span> <span class="o">[</span>info<span class="o">]</span>  Assigned subgraph MERGE_0_4 to rank<span class="o">[</span><span class="m">6</span>,4,0,2<span class="o">]</span>
...
</pre></div>
</div>
<p>The partitioning may take a long time when the model is very large. (It took around five minutes in our environment)</p>
<p>The model was partitioned into two computational graphs (<code class="docutils literal notranslate"><span class="pre">MERGE_0_4</span></code> and <code class="docutils literal notranslate"><span class="pre">MERGE_5_9</span></code>) for model parallelism and they were assigned to rank[6,4,0,2] and ranks[7,5,1,3] respectively for data parallelism
(hybrid model/data parallelism).
Note that RaNNC may set different numbers of replicas for data parallelism for each computational graph to optimize the training throughput.</p>
<p>For more practical usages, <code class="docutils literal notranslate"><span class="pre">test/test_simple.py</span></code> and <a class="reference external" href="https://github.com/nict-wisdom/rannc-examples/">examples</a> will be helpful.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="limitations.html" class="btn btn-neutral float-right" title="Limitations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Data-driven Intelligent System Research Center (DIRECT), National Institute of Information and Communications Technology (NICT).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>